{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras.backend as K\n",
    "import timeit\n",
    "import lightgbm as lgb \n",
    "from sklearn.model_selection import StratifiedKFold \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 30.32 Mb (46.0% reduction)\n",
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n",
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "Shape of train set                 :  (201917, 6)\n",
      "Shape of test set                  :  (123623, 5)\n",
      "Shape of historical_transactions   :  (29112361, 14)\n",
      "Shape of merchants                 :  (334696, 22)\n",
      "Shape of new_merchant_transactions :  (1963031, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1546: RuntimeWarning: overflow encountered in multiply\n",
      "  sqr = np.multiply(arr, arr, out=arr)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  card_id       purchase_date  purchase_amount\n",
      "19095896  C_ID_00007093c1 2017-02-14 14:00:43     6.895150e-07\n",
      "19095775  C_ID_00007093c1 2017-02-14 15:47:45    -4.484254e-04\n",
      "19095845  C_ID_00007093c1 2017-02-16 15:37:58    -5.420467e-04\n",
      "19095866  C_ID_00007093c1 2017-02-20 12:19:01    -3.275201e-04\n",
      "19095808  C_ID_00007093c1 2017-03-03 00:24:15    -4.631373e-04\n",
      "No. of null columns in CustomerLifeValue: \n",
      " card_id            0\n",
      "Frequency          0\n",
      "Monetary           0\n",
      "Age                0\n",
      "Recency            0\n",
      "AvOrderValue       0\n",
      "AgeRecencyRatio    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(pd.read_csv('/kaggle/input/elo-merchant-category-recommendation/train.csv',parse_dates=['first_active_month']))\n",
    "test = reduce_mem_usage(pd.read_csv('/kaggle/input/elo-merchant-category-recommendation/test.csv',parse_dates=['first_active_month']))\n",
    "mer = reduce_mem_usage(pd.read_csv('/kaggle/input/elo-merchant-category-recommendation/merchants.csv'))\n",
    "nmt = reduce_mem_usage(pd.read_csv('/kaggle/input/elo-merchant-category-recommendation/new_merchant_transactions.csv',parse_dates=['purchase_date']))\n",
    "ht = reduce_mem_usage(pd.read_csv('/kaggle/input/elo-merchant-category-recommendation/historical_transactions.csv',parse_dates=['purchase_date']))\n",
    "\n",
    "print(\"Shape of train set                 : \",train.shape)\n",
    "print(\"Shape of test set                  : \",test.shape)\n",
    "print(\"Shape of historical_transactions   : \",ht.shape)\n",
    "print(\"Shape of merchants                 : \",mer.shape)\n",
    "print(\"Shape of new_merchant_transactions : \",nmt.shape)\n",
    "\n",
    "def cleaning(df):\n",
    "  scaler = StandardScaler()\n",
    "  for col in ['authorized_flag', 'category_1']:\n",
    "    df[col] = df[col].map({'Y':1, 'N':0})  \n",
    "    df[col] = df[col].apply(pd.to_numeric, errors='coerce')\n",
    "  for col in ['installments']:\n",
    "    df[col] = df[col].map({-1:14, 0:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:10,11:11,12:12,999:13})\n",
    "    df[col] = df[col].apply(pd.to_numeric, errors='coerce')\n",
    "  for col in ['category_3']:\n",
    "    df[col] = df[col].map({'A':1, 'B':2,'C':3})\n",
    "    df[col] = df[col].apply(pd.to_numeric, errors='coerce')\n",
    "  for col in ['category_2']:\n",
    "    df[col] = df[col].apply(pd.to_numeric, errors='coerce')   \n",
    "  for col in ['purchase_amount']:        \n",
    "    df[col] = scaler.fit_transform(df[[col]])      \n",
    "  return df\n",
    "\n",
    "ht_copy = cleaning(ht.copy())\n",
    "nmt_copy = cleaning(nmt.copy())\n",
    "\n",
    "\n",
    "# Missing values handling\n",
    "for df in [ht_copy, nmt_copy]: # Filling with most common value\n",
    "  df['category_2'].fillna(1,inplace=True)\n",
    "  df['category_3'].fillna(1,inplace=True)\n",
    "  df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "  # Purchase date - year, month, week, hour\n",
    "  df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "  df['year'] = df['purchase_date'].dt.year\n",
    "  df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "  df['month'] = df['purchase_date'].dt.month\n",
    "  df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "  df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "  df['hour'] = df['purchase_date'].dt.hour\n",
    "\n",
    "from datetime import datetime\n",
    "# Here we are trying to calculate recency, frequency, monetary and age.\n",
    "# Recency is how many days back did customer perform a last transaction.\n",
    "# Frequency is how many transactions are performed in time period from dataset.\n",
    "# Monetary is how much was spent in all the transactions.\n",
    "\n",
    "hist = ht_copy[['card_id','purchase_date','purchase_amount']]\n",
    "hist = hist.sort_values(by=['card_id', 'purchase_date'], ascending=[True, True])\n",
    "print(hist.head())\n",
    "\n",
    "z = hist.groupby('card_id')['purchase_date'].max().reset_index()\n",
    "q = hist.groupby('card_id')['purchase_date'].min().reset_index()\n",
    "\n",
    "z.columns = ['card_id', 'Max']\n",
    "q.columns = ['card_id', 'Min']\n",
    "\n",
    "## Extracting current timestamp\n",
    "now = datetime.now()\n",
    "curr_date = now.strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "curr_date = pd.to_datetime(curr_date)\n",
    "\n",
    "rec = pd.merge(z,q,how = 'left',on = 'card_id')\n",
    "rec['Min'] = pd.to_datetime(rec['Min'])\n",
    "rec['Max'] = pd.to_datetime(rec['Max'])\n",
    "\n",
    "## Recency value \n",
    "rec['Recency'] = (curr_date - rec['Max']).astype('timedelta64[D]') ## current date - most recent date\n",
    "\n",
    "## Age value\n",
    "rec['Age'] = (rec['Max'] - rec['Min']).astype('timedelta64[D]') ## Age of customer, MAX - MIN\n",
    "\n",
    "rec = rec[['card_id','Age','Recency']]\n",
    "\n",
    "\n",
    "## Frequency\n",
    "freq = hist.groupby('card_id').size().reset_index()\n",
    "freq.columns = ['card_id', 'Frequency']\n",
    "\n",
    "## Monetary\n",
    "mon = hist.groupby('card_id')['purchase_amount'].sum().reset_index()\n",
    "mon.columns = ['card_id', 'Monetary']\n",
    "\n",
    "final = pd.merge(freq,mon,how = 'left', on = 'card_id')\n",
    "final = pd.merge(final,rec,how = 'left', on = 'card_id')\n",
    "\n",
    "final['AvOrderValue'] = final['Monetary']/final['Frequency'] ## AOV - Average order value (i.e) total_purchase_amt/total_trans\n",
    "final['AgeRecencyRatio'] = final['Age']/final['Recency'] ## \n",
    "\n",
    "print(\"No. of null columns in CustomerLifeValue: \\n\",final.isnull().sum())\n",
    "final.head()\n",
    "\n",
    "\n",
    "ht_copy['purchase_date'] = pd.to_datetime(ht_copy['purchase_date'])\n",
    "ht_copy['month_diff'] = ((datetime.today() - ht_copy['purchase_date']).dt.days)//30\n",
    "ht_copy['month_diff'] += ht_copy['month_lag']\n",
    "\n",
    "nmt_copy['purchase_date'] = pd.to_datetime(nmt_copy['purchase_date'])\n",
    "nmt_copy['month_diff'] = ((datetime.today() - nmt_copy['purchase_date']).dt.days)//30\n",
    "nmt_copy['month_diff'] += nmt_copy['month_lag']\n",
    "\n",
    "hc = ht_copy.isnull().sum()\n",
    "nc = nmt_copy.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_1', 'feature_2', 'feature_3', 'target',\n",
      "       'hist_transactions_count', 'hist_month_mean', 'hist_month_nunique',\n",
      "       'hist_month_diff_mean', 'hist_month_diff_min', 'hist_month_diff_max',\n",
      "       'hist_month_diff_nunique', 'hist_year_nunique', 'hist_year_mean',\n",
      "       'hist_category_1_sum', 'hist_category_1_nunique', 'hist_category_2_sum',\n",
      "       'hist_category_2_nunique', 'hist_category_3_sum',\n",
      "       'hist_category_3_nunique', 'hist_purchase_amount_sum',\n",
      "       'hist_purchase_amount_mean', 'hist_purchase_amount_max',\n",
      "       'hist_purchase_amount_min', 'hist_installments_sum',\n",
      "       'hist_installments_mean', 'hist_installments_max',\n",
      "       'hist_installments_min', 'hist_authorized_flag_nunique',\n",
      "       'hist_month_lag_mean', 'new_transactions_count', 'new_month_mean',\n",
      "       'new_month_nunique', 'new_month_diff_mean', 'new_month_diff_min',\n",
      "       'new_month_diff_max', 'new_month_diff_nunique', 'new_year_nunique',\n",
      "       'new_year_mean', 'new_category_1_sum', 'new_category_1_nunique',\n",
      "       'new_category_2_sum', 'new_category_2_nunique', 'new_category_3_sum',\n",
      "       'new_category_3_nunique', 'new_purchase_amount_sum',\n",
      "       'new_purchase_amount_mean', 'new_purchase_amount_max',\n",
      "       'new_purchase_amount_min', 'new_installments_sum',\n",
      "       'new_installments_mean', 'new_installments_max', 'new_installments_min',\n",
      "       'new_authorized_flag_nunique', 'new_month_lag_mean', 'Frequency',\n",
      "       'Monetary', 'Age', 'Recency', 'AvOrderValue', 'AgeRecencyRatio'],\n",
      "      dtype='object')\n",
      "Train/Test Shape:  (201917, 60) (123623, 59)\n",
      "(179986, 60)\n",
      "y shape:  (179986,)\n",
      "Before drop - X shape:  (179986, 60)\n",
      "After drop - X shape:  (179986, 59)\n",
      "Nulls in X:  0\n",
      "Nulls in y:  0\n",
      "X_train_all, X_test_com, y_train_all, y_test_com shape: (143988, 59) (35998, 59) (143988,) (35998,)\n",
      "X_train_com, X_val_com, y_train_com, y_val_com shape: (115190, 59) (28798, 59) (115190,) (28798,)\n"
     ]
    }
   ],
   "source": [
    "def aggregate_transactions_small(history):\n",
    "    \n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {    \n",
    "        \n",
    "    'month': ['mean','nunique'],     \n",
    "    'month_diff': ['mean', 'min', 'max','nunique'],     \n",
    "    'year': ['nunique', 'mean'],\n",
    "    'category_1' : ['sum',  'nunique'],\n",
    "    'category_2' : ['sum', 'nunique'],\n",
    "    'category_3' : ['sum',  'nunique'],\n",
    "    'purchase_amount': ['sum', 'mean', 'max', 'min'],\n",
    "    'installments': ['sum', 'mean', 'max', 'min'],     \n",
    "    'authorized_flag': ['nunique'],\n",
    "    'month_lag': ['mean']  \n",
    "    }\n",
    "    \n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "    \n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='transactions_count'))\n",
    "    \n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')    \n",
    "    return agg_history\n",
    "  \n",
    "history = aggregate_transactions_small(ht_copy)\n",
    "history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\n",
    "\n",
    "new = aggregate_transactions_small(nmt_copy)\n",
    "new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\n",
    "\n",
    "# Merge all dataframes based on card_id\n",
    "train = pd.merge(train, history, on='card_id', how='left')\n",
    "test = pd.merge(test, history, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, new, on='card_id', how='left')\n",
    "test = pd.merge(test, new, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, final, on='card_id', how='left')\n",
    "test = pd.merge(test, final, on='card_id', how='left')\n",
    "\n",
    "train.drop(['first_active_month','card_id'], inplace=True, axis=1)\n",
    "test.drop(['first_active_month','card_id'], inplace=True, axis=1)\n",
    "print(train.columns)\n",
    "print(\"Train/Test Shape: \",train.shape,test.shape)\n",
    "train.dropna(inplace=True)\n",
    "print(train.shape)\n",
    "y = train['target']\n",
    "print(\"y shape: \",y.shape)\n",
    "print(\"Before drop - X shape: \",train.shape)\n",
    "X = train.drop(['target'], axis=1)\n",
    "print(\"After drop - X shape: \",X.shape)\n",
    "print(\"Nulls in X: \",(X.isnull().sum() > 0 ).sum())\n",
    "print(\"Nulls in y: \",(y.isnull().sum() > 0 ).sum())\n",
    "\n",
    "X_train_all, X_test_com, y_train_all, y_test_com = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"X_train_all, X_test_com, y_train_all, y_test_com shape:\",X_train_all.shape, X_test_com.shape, y_train_all.shape, y_test_com.shape)\n",
    "\n",
    "X_train_com, X_val_com, y_train_com, y_val_com = train_test_split(X_train_all, y_train_all, test_size=0.20, random_state=42)\n",
    "print(\"X_train_com, X_val_com, y_train_com, y_val_com shape:\",X_train_com.shape, X_val_com.shape, y_train_com.shape, y_val_com.shape)\n",
    "\n",
    "#definind the rmse metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Bagging Regressor::3.485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression() \n",
    "regressor.fit(X_train_all, y_train_all) #training the algorithm\n",
    "\n",
    "y_pred_linear = regressor.predict(X_test_com)\n",
    "print(\"RMSE for Bagging Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Bagging Regressor::3.485\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge \n",
    "ridge = Ridge()\n",
    "\n",
    "parameters={'alpha': [1e-3,1e-2]} \n",
    "ridge_regressor=GridSearchCV(ridge,parameters,scoring='neg_root_mean_squared_error',cv=5) \n",
    "ridge_regressor.fit(X_train_all, y_train_all) \n",
    "y_pred_ridge = regressor.predict(X_test_com)\n",
    "print(\"RMSE for Bagging Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_ridge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16084.94275988941, tolerance: 142.6236771594463\n",
      "  positive)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15168.984734155238, tolerance: 144.36849083666996\n",
      "  positive)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10567.541458944557, tolerance: 141.69323739407477\n",
      "  positive)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18466.625813926803, tolerance: 143.11959723356708\n",
      "  positive)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11293.43474023696, tolerance: 141.41178127162527\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Lasso Regressor::3.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17875.575095848413, tolerance: 178.30441120190054\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "lasso = Lasso() \n",
    "parameters={'alpha':[1e-3], 'max_iter':[5000]} \n",
    "lasso_regressor=GridSearchCV(lasso,parameters,scoring='neg_root_mean_squared_error',cv=5)\n",
    "lasso_regressor.fit(X_train_all, y_train_all) \n",
    "y_pred_lasso = lasso_regressor.predict(X_test_com) \n",
    "print(\"RMSE for Lasso Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_lasso)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Decision Tree Regressor::3.475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "treeRegressor = DecisionTreeRegressor()\n",
    "\n",
    "param_grid = {\"criterion\": [\"mse\"], \"max_depth\": [5], \"min_samples_split\": [8], \"max_leaf_nodes\": [15], \"max_features\": [25], \"min_impurity_decrease\":[0.1] } \n",
    "grid_decision = GridSearchCV(treeRegressor, param_grid, cv=3,verbose=1,n_jobs=-1) \n",
    "grid_decision.fit(X_train_all, y_train_all) \n",
    "y_pred_decision = grid_decision.predict(X_test_com)\n",
    "print(\"RMSE for Decision Tree Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_decision)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Random Forest Regressor::3.475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,BaggingRegressor \n",
    "param_grid = {\"criterion\": [\"mse\"], 'n_estimators': [1000], \"max_depth\": [5], \"max_leaf_nodes\" : [5], \"min_samples_split\":[8] , \"max_features\": [25], \"min_impurity_decrease\":[0.1] } \n",
    "forestRegressor = RandomForestRegressor(random_state = 10)\n",
    "\n",
    "grid_forest = GridSearchCV(forestRegressor, param_grid, cv=3, verbose=1,n_jobs=-1) \n",
    "grid_forest.fit(X_train_all, y_train_all)\n",
    "y_pred_forest = grid_forest.predict(X_test_com)\n",
    "print(\"RMSE for Random Forest Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_decision)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   26.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Extra Trees Regressor::3.525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "extraRegressor = ExtraTreesRegressor(random_state = 10) \n",
    "param_grid = {\"criterion\": [\"mse\"], 'n_estimators': [500], \"max_depth\": [5], \"max_leaf_nodes\" : [5], \"min_samples_leaf\":[2], \"min_samples_split\":[2] , \"max_features\": [25], \"min_impurity_decrease\":[0.1] }\n",
    "\n",
    "grid_extra = GridSearchCV(extraRegressor, param_grid, cv=3, verbose=1,n_jobs=-1) \n",
    "grid_extra.fit(X_train_all, y_train_all) \n",
    "y_pred_extra = grid_extra.predict(X_test_com)\n",
    "print(\"RMSE for Extra Trees Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_extra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaboostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Ada Boost Regressor::3.471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor \n",
    "param = { 'n_estimators':[50], 'learning_rate':[1e-2], 'loss':['exponential'] } \n",
    "adaRegressor = AdaBoostRegressor(random_state = 10)\n",
    "\n",
    "grid_ada = GridSearchCV(adaRegressor, param, cv = 3, n_jobs = -1, verbose=1) \n",
    "grid_ada.fit(X_train_all, y_train_all) \n",
    "y_pred_ada = grid_ada.predict(X_test_com)\n",
    "print(\"RMSE for Ada Boost Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_ada)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Gradient Boost Regressor::3.425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "param_grid = {'n_estimators': [100]} \n",
    "gbRegressor = GradientBoostingRegressor(random_state = 10)\n",
    "\n",
    "grid_gb = GridSearchCV(gbRegressor, param_grid, cv=3, verbose=1,n_jobs=-1) \n",
    "grid_gb.fit(X_train_all, y_train_all) \n",
    "y_pred_gb = grid_gb.predict(X_test_com)\n",
    "print(\"RMSE for Gradient Boost Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_gb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  5.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:20:59] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { criterion, max_features } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Ensemble Executed in 6.062381015150004 minutes\n",
      "RMSE for XG Boost Regressor::3.425\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from xgboost import XGBRegressor\n",
    "start = timeit.default_timer()\n",
    "xgb = XGBRegressor() \n",
    "parameters = { 'gamma': [8], 'eval_metric' :['rmse'],'eta': [0.5], 'colsample_bytree':[0.3], 'min_child_weight': [3], 'max_depth' :[3], 'max_features':[5],'subsample': [0.7],'tree_method':['auto'], 'reg_alpha':[1000], \"criterion\": [\"mse\"],'n_estimators': [1000] ,'seed':[11] }\n",
    "\n",
    "grid_xgb = GridSearchCV(xgb, parameters, cv = 6, n_jobs = -1, verbose=1) \n",
    "grid_xgb.fit(X_train_all, y_train_all) \n",
    "y_pred_xgb = grid_xgb.predict(X_test_com)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "execution_time = (stop - start)/60 \n",
    "print(\"Ensemble Executed in {} minutes\".format(str(execution_time)))\n",
    "print(\"RMSE for XG Boost Regressor::{:.3f}\".format(rmse(y_test_com, y_pred_gb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, X_test, y_train, y_test shape: (143988, 59) (35998, 59) (143988,) (35998,)\n",
      "Fitting models to meta-learner.\n",
      "Preparing Custom Ensemble...\n",
      "D1_train, D1_test, D2_train, D2_test:  (71994, 59) (71994, 59) (71994,) (71994,)\n",
      "\n",
      "Calculating Samples with replacement...\n",
      "Samples calculation Done.\n",
      "Sample size:  720\n",
      "Base models 100 - fitting and predicting ...\n",
      "Base models done.\n",
      "Custom Ensemble Done.\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  2.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:25:00] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { criterion, max_features } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Predicting 100 models from metalearner...\n",
      "Meta Learner prediction Done.\n",
      "Evaluating Score...\n",
      "\n",
      "RMSE 1.859\n",
      "Ensemble Executed in 3.5083673163833358 minutes\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "print(\"X_train, X_test, y_train, y_test shape:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "SEED = 10 \n",
    "\n",
    "#Using Decision Tree as a base model\n",
    "def get_model():\n",
    "    dt = DecisionTreeRegressor(criterion = 'mse', max_depth = 5, max_features = 50, max_leaf_nodes = 15, min_impurity_decrease = 0.1, random_state=SEED)\n",
    "    return dt\n",
    "\n",
    "from random import choices \n",
    "\n",
    "def get_samples(D1_train, D2_train, n_estimators): \n",
    "    print(\"\\nCalculating Samples with replacement...\") \n",
    "    samples_train_appender = [] \n",
    "    samples_test_appender = [] \n",
    "    all_indexes = D1_train.index \n",
    "    population_size = D1_train.shape[0]\n",
    "    sample_size = round(population_size/n_estimators)\n",
    "    bumpedup_sample_size = int(sample_size * 1.85)\n",
    "\n",
    "    for s in range(n_estimators): \n",
    "        samples_index = choices(all_indexes, k = sample_size)\n",
    "        sample_train_df = D1_train[D1_train.index.isin(samples_index)] \n",
    "        sample_test_df = D2_train[D2_train.index.isin(samples_index)]\n",
    "        samples_train_appender.append(sample_train_df) \n",
    "        samples_test_appender.append(sample_test_df)\n",
    "\n",
    "    all_train_samples = pd.concat(samples_train_appender,ignore_index=True) \n",
    "    all_test_samples = pd.concat(samples_test_appender,ignore_index=True) \n",
    "    print(\"Samples calculation Done.\")\n",
    "    return all_train_samples, all_test_samples, sample_size\n",
    "\n",
    "#Calculate RMSE\n",
    "def evaluate_model(y_pred, y_actual): \n",
    "    print(\"Evaluating Score...\\n\") \n",
    "    mse = rmse(y_actual, y_pred) \n",
    "    print('RMSE %.3f' % (np.sqrt(mse)))\n",
    "\n",
    "def train_predict(n_estimators, all_train_samples, all_test_samples, D1_test,D2_test, sample_size): \n",
    "    \"\"\"Fit models in list on training set and return preds\"\"\" \n",
    "    P = np.zeros((D2_test.shape[0], n_estimators)) \n",
    "    P = pd.DataFrame(P) \n",
    "    models_list = []\n",
    "\n",
    "    print(\"Sample size: \", sample_size) \n",
    "    cols = list() \n",
    "    print(\"Base models {} - fitting and predicting ...\".format(n_estimators))\n",
    "\n",
    "    for i in range(0,n_estimators):\n",
    "        j = sample_size * i \n",
    "        k = sample_size * (i + 1) \n",
    "        x_data = all_train_samples[j:k] \n",
    "        y_data = all_test_samples[j:k] \n",
    "        model = get_model()\n",
    "\n",
    "        model.fit(x_data, y_data)  \n",
    "        models_list.append(model)           \n",
    "        pred = (model.predict(D1_test))        \n",
    "        P.iloc[:,i] = pred\n",
    "        cols.append(i)\n",
    "    P.columns = cols \n",
    "    print(\"Base models done.\") \n",
    "    return P, models_list\n",
    "\n",
    "def custom_ensemble(X_train,y_train,n_estimators): \n",
    "    print(\"Preparing Custom Ensemble...\")\n",
    "\n",
    "    #Split X_train into D1,D2 (50-50) \n",
    "    D1_train, D1_test, D2_train, D2_test = train_test_split(X_train, y_train, test_size=0.5, random_state=42) \n",
    "    print(\"D1_train, D1_test, D2_train, D2_test: \",D1_train.shape, D1_test.shape, D2_train.shape, D2_test.shape)\n",
    "\n",
    "    #Get Samples \n",
    "    all_samples_train, all_samples_test, sample_size = get_samples(D1_train, D2_train, n_estimators)\n",
    "\n",
    "    #Get predictions \n",
    "    P, models_list = train_predict(n_estimators, all_samples_train, all_samples_test,D1_test, D2_test, sample_size) \n",
    "    print(\"Custom Ensemble Done.\") \n",
    "    return P, models_list, D2_test\n",
    "\n",
    "def super_train_predict(n_estimators, models_list, test_set): \n",
    "    \"\"\"Fit models in list on training set and return preds\"\"\" \n",
    "    meta_pred = np.zeros((test_set.shape[0] , n_estimators)) \n",
    "    meta_pred = pd.DataFrame(meta_pred)\n",
    "\n",
    "    print(\"Predicting {} models from metalearner...\".format(n_estimators)) \n",
    "    cols = list() \n",
    "    for i in range(0,n_estimators):\n",
    "        model = models_list[i]\n",
    "        pred = (model.predict(test_set))\n",
    "        meta_pred.iloc[:,i] = pred \n",
    "        cols.append(i)\n",
    "    \n",
    "    meta_pred.columns = cols\n",
    "    print(\"Meta Learner prediction Done.\") \n",
    "    return meta_pred\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "xgb = XGBRegressor() \n",
    "parameters = { 'gamma': [8], 'eval_metric' :['rmse'],'eta': [0.5], 'colsample_bytree':[0.3], 'min_child_weight': [3], 'max_depth' :[3], 'max_features':[5],'subsample': [0.7],'tree_method':['auto'], 'reg_alpha':[1000], \"criterion\": [\"mse\"],'n_estimators': [1000] ,'seed':[11] }\n",
    "\n",
    "meta_learner = GridSearchCV(xgb, parameters, cv = 6, n_jobs = -1, verbose=1)\n",
    "\n",
    "n_estimators = 100 \n",
    "print(\"Fitting models to meta-learner.\") \n",
    "P, models_list, D2_test = custom_ensemble(X_train, y_train, n_estimators) \n",
    "meta_learner.fit(P, D2_test)\n",
    "\n",
    "# Ensemble final prediction and evaluation\n",
    "meta_pred = super_train_predict(n_estimators, models_list, X_test) # X_test brought from first split \n",
    "pred_final = meta_learner.predict(meta_pred)\n",
    "\n",
    "rmse = evaluate_model(pred_final,y_test)\n",
    "\n",
    "stop = timeit.default_timer() \n",
    "execution_time = (stop - start)/60\n",
    "\n",
    "print(\"Ensemble Executed in {} minutes\".format(str(execution_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.5692\n",
      "Epoch 00001: val_loss improved from inf to 1591.00891, saving model to weights.hdf5\n",
      "592/592 [==============================] - 114s 193ms/step - loss: 1.5692 - val_loss: 1591.0089\n",
      "Epoch 2/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4808\n",
      "Epoch 00002: val_loss improved from 1591.00891 to 40.64721, saving model to weights.hdf5\n",
      "592/592 [==============================] - 112s 189ms/step - loss: 1.4808 - val_loss: 40.6472\n",
      "Epoch 3/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4768\n",
      "Epoch 00003: val_loss improved from 40.64721 to 1.62873, saving model to weights.hdf5\n",
      "592/592 [==============================] - 112s 190ms/step - loss: 1.4768 - val_loss: 1.6287\n",
      "Epoch 4/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4756\n",
      "Epoch 00004: val_loss improved from 1.62873 to 1.56361, saving model to weights.hdf5\n",
      "592/592 [==============================] - 112s 189ms/step - loss: 1.4756 - val_loss: 1.5636\n",
      "Epoch 5/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4748\n",
      "Epoch 00005: val_loss did not improve from 1.56361\n",
      "592/592 [==============================] - 111s 187ms/step - loss: 1.4748 - val_loss: 15313.5020\n",
      "Epoch 6/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4731\n",
      "Epoch 00006: val_loss improved from 1.56361 to 1.53994, saving model to weights.hdf5\n",
      "592/592 [==============================] - 115s 195ms/step - loss: 1.4731 - val_loss: 1.5399\n",
      "Epoch 7/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4724\n",
      "Epoch 00007: val_loss did not improve from 1.53994\n",
      "592/592 [==============================] - 113s 191ms/step - loss: 1.4724 - val_loss: 1.7341\n",
      "Epoch 8/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4728\n",
      "Epoch 00008: val_loss did not improve from 1.53994\n",
      "592/592 [==============================] - 117s 197ms/step - loss: 1.4728 - val_loss: 1.5479\n",
      "Epoch 9/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4727\n",
      "Epoch 00009: val_loss did not improve from 1.53994\n",
      "592/592 [==============================] - 115s 194ms/step - loss: 1.4727 - val_loss: 1.5498\n",
      "Epoch 10/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4708\n",
      "Epoch 00010: val_loss did not improve from 1.53994\n",
      "592/592 [==============================] - 119s 201ms/step - loss: 1.4708 - val_loss: 3.3766\n",
      "Epoch 11/15\n",
      "592/592 [==============================] - ETA: 0s - loss: 1.4707\n",
      "Epoch 00011: val_loss did not improve from 1.53994 \n",
      "592/592 [==============================] - 122s 206ms/step - loss: 1.4707 - val_loss: 1.590\n",
      "Early Stopping... \n",
      "RMSE for NN::{:.3f}: 3.4278\n"
     ]
    }
   ],
   "source": [
    "X_nn_train, X_nn_val, y_nn_train, y_nn_val = train_test_split(X, y, test_size=0.25, random_state=42) \n",
    "print(\"X_train, y_train, X_val, y_val Shape: \",X_nn_train.shape, y_nn_train.shape, X_nn_val.shape, y_nn_val.shape)\n",
    "\n",
    "test_min = test_df[important_cols]\n",
    "test_min.replace({np.inf: 0, -np.inf: 0}, inplace=True)\n",
    "imputer_test = SimpleImputer(missing_values=np.nan,strategy='median')\n",
    "imputer_test = imputer_test.fit(test_min)\n",
    "test_nn = imputer_test.transform(test_min)\n",
    "print(\"test_nn.shape: \",test_nn.shape)\n",
    "\n",
    "# Scaling data\n",
    "sc = StandardScaler()\n",
    "X_nn_train = sc.fit_transform(X_nn_train)\n",
    "test_nn = sc.transform(test_nn)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "\n",
    "#definind the rmse metric\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "fh_neurons = 1024 #first hidden layer\n",
    "drop_rate = 0.7\n",
    "\n",
    "#the model is just a sequence of fully connected layers, batch normalization and dropout using RELUs as activation functions\n",
    "nmodel = Sequential()\n",
    "nmodel.add(Dense(fh_neurons, input_dim=X_nn_train.shape[1], activation='elu'))\n",
    "nmodel.add(BatchNormalization())\n",
    "nmodel.add(Dropout(drop_rate))\n",
    "nmodel.add(Dense(fh_neurons*2, activation='relu'))\n",
    "nmodel.add(BatchNormalization())\n",
    "nmodel.add(Dropout(drop_rate))\n",
    "nmodel.add(Dense(fh_neurons*2, activation='relu'))\n",
    "nmodel.add(Dense(fh_neurons, activation='relu'))\n",
    "nmodel.add(Dense(1, activation='linear'))\n",
    "\n",
    "nmodel.compile(optimizer='adam',loss=rmse)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "nmodel.fit(X_nn_train, y_nn_train, validation_data = (X_nn_val, y_nn_val), epochs=15, batch_size=256, callbacks = [early_stopping, checkpointer])\n",
    "pred_nn = nmodel.predict(test_nn)\n",
    "print(\"RMSE for NN::{:.3f}\".format(rmse(y_te, y_pred_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " =========== RMSE of Various Models ==========\n",
      "\n",
      "+-------+-------------------------+--------+\n",
      "| S No. |        Model name       |  RMSE  |\n",
      "+-------+-------------------------+--------+\n",
      "|   1   |    Linear Regression    | 3.485  |\n",
      "|   2   |     Ridge Regression    | 3.485  |\n",
      "|   3   |     Lasso Regression    | 3.492  |\n",
      "|   4   | Decision Tree Regressor | 3.475  |\n",
      "|   5   | Random Forest Regressor | 3.475  |\n",
      "|   6   |  Extra Trees Regressor  | 3.525  |\n",
      "|   7   |    XGBoost Regressor    | 3.425  |\n",
      "|   8   |    AdaBoost Regressor   | 3.471  |\n",
      "|   9   | GradientBoost Regressor | 3.425  |\n",
      "|   10  |         Ensemble        | 1.859  |\n",
      "|   11  |     Neural Networks     | 3.4278 |\n",
      "+-------+-------------------------+--------+\n",
      "Ensemble score is lowest at nearly half of other scores !!!\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "print(\"\\n =========== RMSE of Various Models ==========\\n\")\n",
    "\n",
    "af = PrettyTable()\n",
    "\n",
    "af.field_names = [\"S No.\",\"Model name\", \"RMSE\"]\n",
    "\n",
    "af.add_row([\"1\",\"Linear Regression\", 3.485])\n",
    "af.add_row([\"2\",\"Ridge Regression\", 3.485])\n",
    "af.add_row([\"3\",\"Lasso Regression\", 3.492])\n",
    "af.add_row([\"4\",\"Decision Tree Regressor\", 3.475])\n",
    "af.add_row([\"5\",\"Random Forest Regressor\", 3.475])\n",
    "af.add_row([\"6\",\"Extra Trees Regressor\", 3.525])\n",
    "af.add_row([\"7\",\"XGBoost Regressor\", 3.425])\n",
    "af.add_row([\"8\",\"AdaBoost Regressor\", 3.471])\n",
    "af.add_row([\"9\",\"GradientBoost Regressor\", 3.425])\n",
    "af.add_row([\"10\",\"Ensemble\", 1.859])\n",
    "af.add_row([\"11\",\"Neural Networks\", 3.4278])\n",
    "print(af)\n",
    "print(\"Ensemble score is lowest at nearly half of other scores !!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
